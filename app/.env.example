# Server Configuration
PORT=8000
ENV=development

# Security
API_KEY=your-secret-key-here

# Model Configuration
USE_MOCK=false  # Set to 'true' to use mocked inference (for CI/staging without GPU)

# RAG Configuration
TAVILY_API_KEY=your-tavily-api-key-here  # Get from https://tavily.com

# Ollama LLM Judge Configuration
OLLAMA_BASE_URL=http://localhost:11434  # Local Ollama server
OLLAMA_MODEL=llama3.1:8b-instruct-q4_K_M  # Model for domain classification

# Docker/Registry (for CI/CD)
REGISTRY_IMAGE=ghcr.io/your-username/fastapi-llm-inference
STAGE=staging
